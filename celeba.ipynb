{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-20T17:29:43.000356328Z",
     "start_time": "2023-12-20T17:29:41.069961258Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['image_id', '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive',\n       'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose',\n       'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows',\n       'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair',\n       'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open',\n       'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin',\n       'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns',\n       'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings',\n       'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace',\n       'Wearing_Necktie', 'Young'],\n      dtype='object')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/celeba/list_attr_celeba.csv')\n",
    "df.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T17:29:43.261534304Z",
     "start_time": "2023-12-20T17:29:43.000588123Z"
    }
   },
   "id": "da63b15b7678a034"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            img_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.attr_frame = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.attr_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.attr_frame.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        attributes = self.attr_frame.iloc[idx, 1:].to_numpy()\n",
    "        attributes = torch.from_numpy(attributes.astype('float')).float()\n",
    "\n",
    "        return image, attributes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T17:29:43.263293395Z",
     "start_time": "2023-12-20T17:29:43.261880246Z"
    }
   },
   "id": "91f9a591160c0a36"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class ConditionalConvVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    条件全卷积VAE变分自动编码器\n",
    "    \"\"\"\n",
    "    NAME = 'ConditionalConvVAE'\n",
    "    \n",
    "    def __init__(self, potential_dim, channels, num_attributes=40):\n",
    "        super(ConditionalConvVAE, self).__init__()\n",
    "        self.potential_dim = potential_dim\n",
    "        self.channels = channels\n",
    "        \n",
    "        # 对类别标签进行编码的线性层\n",
    "        self.attr_embedding = nn.Linear(num_attributes, num_attributes)\n",
    "        \n",
    "        output_shape = (128, 6, 7)\n",
    "        \n",
    "        output_dim = output_shape[0] * output_shape[1] * output_shape[2]\n",
    "        # image_size = (178, 218)\n",
    "        # 编码器\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(channels + num_attributes, 64, kernel_size=3, stride=2, padding=1),  # 89\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),  # 45\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),  # 23\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),  # 12\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),  # 6\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.enc_mu = nn.Linear(output_dim, potential_dim)     # 均值\n",
    "        self.enc_log_var = nn.Linear(output_dim, potential_dim) # 对数方差\n",
    "        # 解码器\n",
    "        self.decoder_fc = nn.Linear(potential_dim + num_attributes, output_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, output_shape),  # 6\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),  # 12\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=0),\n",
    "            nn.ReLU(),  # 23\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=3, stride=2, padding=1, output_padding=0),\n",
    "            nn.ReLU(),  # 45\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=3, stride=2, padding=1, output_padding=0),\n",
    "            nn.ReLU(),  # 89\n",
    "            nn.ConvTranspose2d(64, channels, kernel_size=3, stride=2, padding=1, output_padding=1), \n",
    "            nn.AdaptiveAvgPool2d((178, 218)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x, attributes):\n",
    "        # 将标签嵌入到与图像相同的维度\n",
    "        # print(x.shape)  # torch.Size([128, 3, 178, 218])\n",
    "        embedded_attrs = self.attr_embedding(attributes).unsqueeze(2).unsqueeze(3)\n",
    "        embedded_attrs = embedded_attrs.expand(embedded_attrs.size(0), embedded_attrs.size(1), x.size(2), x.size(3))\n",
    "        \n",
    "        # 将标签和图像连接起来\n",
    "        x = torch.cat((x, embedded_attrs), dim=1)\n",
    "        \n",
    "        # 传入编码器\n",
    "        x = self.encoder(x)\n",
    "        # print(x.shape)\n",
    "        mu = self.enc_mu(x)\n",
    "        log_var = self.enc_log_var(x)\n",
    "        \n",
    "        # 重参数化\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        \n",
    "        return z, mu, log_var\n",
    "\n",
    "    def decode(self, z, labels):\n",
    "        # 将标签嵌入并与潜在向量连接起来\n",
    "        labels = self.attr_embedding(labels)\n",
    "        z = torch.cat((z, labels), dim=1)\n",
    "        \n",
    "        # 传入解码器\n",
    "        x = self.decoder_fc(z)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        z, mu, log_var = self.encode(x, labels)\n",
    "        reconstructed_x = self.decode(z, labels)\n",
    "        return reconstructed_x, mu, log_var\n",
    "\n",
    "\n",
    "# 定义损失函数\n",
    "def vae_loss(recon_x, x, mu, log_var):\n",
    "    # print(recon_x.shape, x.shape)\n",
    "    batch_size = recon_x.size(0)\n",
    "    # 重构损失：通常使用二元交叉熵（BCE）损失\n",
    "    MSE = nn.functional.mse_loss(recon_x.view(batch_size, -1), x.view(batch_size, -1), reduction='sum')\n",
    "    # KL 散度损失：用于度量学到的潜在分布与标准正态分布之间的差异\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    loss = MSE + KLD\n",
    "    return loss, np.array([loss.item(), MSE.item(), KLD.item()])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T17:29:43.312220180Z",
     "start_time": "2023-12-20T17:29:43.268018047Z"
    }
   },
   "id": "2ab23628610ce982"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看可用的训练设备\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T17:29:43.328813336Z",
     "start_time": "2023-12-20T17:29:43.290408069Z"
    }
   },
   "id": "e2bb2fce22e119ed"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class DatasetType(Enum):\n",
    "    cifar10 = 'cifar10'\n",
    "    mnist = 'mnist'\n",
    "    fashion_mnist = 'fashion_mnist'\n",
    "    svhn = 'svhn'\n",
    "    celeba = 'celeba'\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T17:29:43.364939060Z",
     "start_time": "2023-12-20T17:29:43.324955915Z"
    }
   },
   "id": "c0ecf97ae2471ce0"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# 训练配置\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "potential_dim = 64\n",
    "dataset_type = DatasetType.celeba  # 在这里设置你的数据集"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T17:29:43.376919225Z",
     "start_time": "2023-12-20T17:29:43.350437270Z"
    }
   },
   "id": "52d87de97b336989"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "MODEL_PATH = f'models/cvae_{dataset_type.value}.pth'\n",
    "HISTORY_PATH = f'history/cvae_{dataset_type.value}_history.npy'\n",
    "CHANNELS = 1 if dataset_type in [DatasetType.mnist, DatasetType.fashion_mnist] else 3\n",
    "\n",
    "# 模型和优化器\n",
    "vae = ConditionalConvVAE(potential_dim=potential_dim, channels=CHANNELS)\n",
    "vae.to(device)\n",
    "\n",
    "running_losses = []\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    try:\n",
    "        vae.load_state_dict(torch.load(MODEL_PATH))\n",
    "        running_losses = list(np.load(HISTORY_PATH)) if os.path.exists(HISTORY_PATH) else []\n",
    "    except RuntimeError:\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T17:29:43.476619844Z",
     "start_time": "2023-12-20T17:29:43.376566552Z"
    }
   },
   "id": "a243cddd5f198c22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:39<?, ?it/s, KLDLoss=8177.341, MSELoss=932155.725, progress=7.90%, totalLoss=940333.067]  "
     ]
    }
   ],
   "source": [
    "images_path = 'data/celeba/img_align_celeba/img_align_celeba'\n",
    "image_size = (178, 218)\n",
    "\n",
    "# Usage\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = CelebADataset(csv_file='data/celeba/list_attr_celeba.csv',\n",
    "                               img_dir='data/celeba/img_align_celeba/img_align_celeba',\n",
    "                               transform=transform)\n",
    "\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.001)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "p_bar = tqdm(range(epochs))\n",
    "for epoch in p_bar:\n",
    "    running_loss = np.array([0., 0., 0.])\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, log_var = vae(data, labels)\n",
    "        loss, losses = vae_loss(recon_batch, data, mu, log_var)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += 1 / (batch_idx + 1) * (losses - running_loss)\n",
    "        p_bar.set_postfix(progress=f'{(batch_idx + 1) / len(train_loader) * 100:.2f}%',\n",
    "                          totalLoss=f'{running_loss[0]:.3f}', MSELoss=f'{running_loss[1]:.3f}',\n",
    "                          KLDLoss=f'{running_loss[2]:.3f}')\n",
    "\n",
    "    running_losses.append(running_loss)\n",
    "    np.save(HISTORY_PATH, np.array(running_losses))\n",
    "    torch.save(vae.state_dict(), MODEL_PATH)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-12-20T17:29:43.479319140Z"
    }
   },
   "id": "953372192ba41651"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6823e7c4304c38f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
